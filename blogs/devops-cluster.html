<!DOCTYPE html><html lang="en"><head><meta name="theme-color" content="#1976d2"/><meta charSet="utf-8"/><meta name="viewport" content="minimum-scale=1, initial-scale=1, width=device-width"/><title>DevOps集群</title><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/b192415249a5ea5e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b192415249a5ea5e.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-1fec710eb11e1257.js" defer=""></script><script src="/_next/static/chunks/framework-0f8b31729833af61.js" defer=""></script><script src="/_next/static/chunks/main-e7f9cd3517807e31.js" defer=""></script><script src="/_next/static/chunks/pages/_app-e54de001e0818968.js" defer=""></script><script src="/_next/static/chunks/650-2774057cbd5d0239.js" defer=""></script><script src="/_next/static/chunks/301-4caaf5c3df1c8949.js" defer=""></script><script src="/_next/static/chunks/pages/blogs/%5Bslug%5D-0a1bfc61bcd0f7ce.js" defer=""></script><script src="/_next/static/oTnaln6XuL20mMnIg8gCL/_buildManifest.js" defer=""></script><script src="/_next/static/oTnaln6XuL20mMnIg8gCL/_ssgManifest.js" defer=""></script><script src="/_next/static/oTnaln6XuL20mMnIg8gCL/_middlewareManifest.js" defer=""></script><style data-emotion="css "></style></head><body><div id="__next"><style data-emotion="css-global o6gwfi">html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;box-sizing:border-box;-webkit-text-size-adjust:100%;}*,*::before,*::after{box-sizing:inherit;}strong,b{font-weight:700;}body{margin:0;color:rgba(0, 0, 0, 0.87);font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:400;font-size:1rem;line-height:1.5;letter-spacing:0.00938em;background-color:#fff;}@media print{body{background-color:#fff;}}body::backdrop{background-color:#fff;}</style><section class="css-1vqbnt3"><header class="MuiPaper-root MuiPaper-elevation MuiPaper-elevation4 MuiAppBar-root MuiAppBar-colorPrimary MuiAppBar-positionStatic css-1x7skt0"><div class="MuiToolbar-root MuiToolbar-gutters MuiToolbar-regular css-191lty2"><div status="[object Object]" class="css-ad3hhb"></div><button class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit MuiIconButton-sizeLarge css-1l1167e" tabindex="0" type="button" aria-label="Menu"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-vubbuv" focusable="false" viewBox="0 0 24 24" aria-hidden="true" data-testid="MenuIcon"><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></button><p class="MuiTypography-root MuiTypography-body1 css-1ikde92" type="title"><button class="MuiButton-root MuiButton-text MuiButton-textInherit MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-colorInherit MuiButtonBase-root  css-b7766g" tabindex="0" type="button"><a href="/">Home</a></button><button class="MuiButton-root MuiButton-text MuiButton-textInherit MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-colorInherit MuiButtonBase-root  css-b7766g" tabindex="0" type="button"><a href="/blogs">Blog</a></button><button class="MuiButton-root MuiButton-text MuiButton-textInherit MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-colorInherit MuiButtonBase-root  css-b7766g" tabindex="0" type="button"><a href="/presentations">Presentation</a></button><button class="MuiButton-root MuiButton-text MuiButton-textInherit MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-colorInherit MuiButtonBase-root  css-b7766g" tabindex="0" type="button"><a href="/tools">Tool</a></button><button class="MuiButton-root MuiButton-text MuiButton-textInherit MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-colorInherit MuiButtonBase-root  css-b7766g" tabindex="0" type="button"><a href="/games">Game</a></button></p></div></header><section class="css-xxwux"><div class="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation4 css-gnirli"><header><h1 class="MuiTypography-root MuiTypography-body1 css-9l3uo3">DevOps集群</h1><div class="MuiTypography-root MuiTypography-body1 css-9l3uo3">Author:<!-- -->Gao</div><div class="MuiTypography-root MuiTypography-body1 css-9l3uo3">Created At:<!-- -->2020-05-12</div></header><hr class="MuiDivider-root MuiDivider-fullWidth css-39bbo6"/><section class="MuiTypography-root MuiTypography-body1 blog-content css-9l3uo3"><p>最近配置了一个 DevOps 集群给前端使用，现在整理一下这个流程，和在做成碰到的问题</p>
<h3>安装</h3>
<h4>集群</h4>
<p>由于要配置多台服务器，安装使用离线安装，来减轻现在的负担</p>
<p>下载好 k3s 和镜像包</p>
<ul>
<li>master:</li>
</ul>
<pre><code class="language-bash">cp ./k3s /usr/local/bin/
cp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/

cat ./install.sh | \
INSTALL_K3S_SKIP_DOWNLOAD=true \
    sh -s - server \
    --cluster-domain=devops.local \
    --cluster-cidr=10.31.0.0/16 \
    --service-cidr=10.32.0.0/16 \
    --default-local-storage-path=/storage/volumes \
    --node-name=master \
    --docker
</code></pre>
<ul>
<li>backup:</li>
</ul>
<pre><code class="language-bash">mkdir -p /var/lib/rancher/k3s/agent/images/
cp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/

export K3S_TOKEN=&quot;{server-token}&quot;
export K3S_URL=&quot;https://{server-ip}:6443&quot;

cat ./install.sh | \
INSTALL_K3S_SKIP_DOWNLOAD=true \
    sh -s - agent \
    --docker \
    --node-name=slave \
</code></pre>
<ul>
<li>⚠️ 注意问题</li>
</ul>
<ul>
<li>安装好后集群运行有问题，发现是 iptables 设置问题处理方式</li>
</ul>
<pre><code class="language-bash"># master
iptables -F &amp;&amp; systemctl restart k3s
# slave
iptables -F &amp;&amp; systemctl restart k3s-agent
</code></pre>
<p>如果服务重启或者网络重启，可能会导致问题，也需要这么处理</p>
<ul>
<li>这里使用 docker 而没有使用 containerd 是因为网络环境有问题，rancher-agent 无法
运行导致的</li>
<li>安装完成后禁止了在 master 运行任务，防止 master 负荷过大导致集群瘫痪
<code>kubectl taint node mymasternode node-role.kubernetes.io/master:NoSchedule</code></li>
</ul>
<h4>安装 Rancher</h4>
<p>首先安装 helm，下载到 <code>/usr/local/bin/helm</code></p>
<p>安装 rancher</p>
<pre><code class="language-shell">helm repo add rancher  https://releases.rancher.com/server-charts/latest
helm repo update
helm install rancher rancher-stable/rancher --namespace cattle-system
</code></pre>
<p>配置好域名证书后，正常访问</p>
<h4>NFS</h4>
<p>希望集群能够共享数据，所以增加了 nfs 存储，可以在节点之间共享数据</p>
<ul>
<li>配置 NFS 服务器</li>
</ul>
<p>方便管理，将所有 export 都放入一个目录，真正的目录都通过<code>--bind</code>来绑定</p>
<pre><code class="language-shell">mkdir -p /export/volumes
# 配置权限
chmod 777 -R /export

mount --bind /home/nfs-data /export/volumes
</code></pre>
<p>在/etc/fstab 中添加</p>
<pre><code class="language-fstab">/home/nfs-data    /export/volumes   none    bind  0  0
</code></pre>
<p>在<code>/etc/exports</code>中配置</p>
<pre><code class="language-exports">/export/volumes 10.1.108.0/24(rw,nohide,subtree_check,insecure,all_squash,anonuid=0,async)

/export 10.1.0.0/16(ro,fsid=0,root_squash,no_subtree_check,hide)
</code></pre>
<h4>安装 nfs-client-provisioner</h4>
<p>使用 helm 安装 nfs-client-provisioner，为集群提供动态存储</p>
<pre><code class="language-shell">helm install -n kube-system nfs-client-provisioner --set nfs.server={nfs-server-ip} --set nfs.path=/volumes stable/nfs-client-provisioner --set storageClass.name=nfs
</code></pre>
<h4>安装 Tekton</h4>
<p>配置安装 Tekton</p>
<p>前置工作已经完成，使用 github 上的项目来安装</p>
<p>下载目录下文件 <code>https://github.com/gsmlg/pipeline/tree/master/tektoncd</code></p>
<p>直接安装即可</p>
<p>需要配置项目</p>
<ul>
<li>ssh-key 配置 known_hosts 和 ssh-privatekey</li>
</ul>
<p>获取权限</p>
<p>这个项目中定义了一个 pipeline，用于跑项目任务</p>
<p>在 rancher 和 gitlab 配置好 eventlistener 的触发 URL，就可以自动运行当前的
pipeline 了</p>
<h3>总结问题</h3>
<p>过程中碰到 too many open files 的问题，修改 ulimit 解决</p>
<p>测试网络和服务器不太稳定，经常连接不上，或者运行两个任务后，系统很卡，主要是虚拟
平台给的性能严重不足</p>
<p>nfs 服务器和集群机器之间网络有问题，导致无法共享存储，最后被迫使用了
local-path，失去了共享存储的并行任务能力</p>
<p>碰到 tekton 升级 0.12 版本，更新了 git 资源到 task 来实现共享工作空间</p>
<p>给 nfs，tekton，rancher 增加了 toleration 和 affinity，将他们调度到 master，防止
worker 过于繁忙，导致的无法服务问题</p></section></div></section><footer style="margin-top:0" class="css-6haedj"><div class="container"><span id="icp-info" class="css-z2a085">京ICP备20014476号-2</span><span>Copyright © 2017-2021 GSMLG - Powered by GSMLG Web.</span></div></footer></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"devops-cluster","blog":{"id":37,"name":"devops-cluster","title":"DevOps集群","date":"2020-05-12","author":"Gao","content":"最近配置了一个 DevOps 集群给前端使用，现在整理一下这个流程，和在做成碰到的问题\n\n### 安装\n\n#### 集群\n\n由于要配置多台服务器，安装使用离线安装，来减轻现在的负担\n\n下载好 k3s 和镜像包\n\n- master:\n\n```bash\ncp ./k3s /usr/local/bin/\ncp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/\n\ncat ./install.sh | \\\nINSTALL_K3S_SKIP_DOWNLOAD=true \\\n    sh -s - server \\\n    --cluster-domain=devops.local \\\n    --cluster-cidr=10.31.0.0/16 \\\n    --service-cidr=10.32.0.0/16 \\\n    --default-local-storage-path=/storage/volumes \\\n    --node-name=master \\\n    --docker\n```\n\n- backup:\n\n```bash\nmkdir -p /var/lib/rancher/k3s/agent/images/\ncp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/\n\nexport K3S_TOKEN=\"{server-token}\"\nexport K3S_URL=\"https://{server-ip}:6443\"\n\ncat ./install.sh | \\\nINSTALL_K3S_SKIP_DOWNLOAD=true \\\n    sh -s - agent \\\n    --docker \\\n    --node-name=slave \\\n```\n\n- ⚠️ 注意问题\n\n* 安装好后集群运行有问题，发现是 iptables 设置问题处理方式\n\n```bash\n# master\niptables -F \u0026\u0026 systemctl restart k3s\n# slave\niptables -F \u0026\u0026 systemctl restart k3s-agent\n```\n\n如果服务重启或者网络重启，可能会导致问题，也需要这么处理\n\n- 这里使用 docker 而没有使用 containerd 是因为网络环境有问题，rancher-agent 无法\n  运行导致的\n- 安装完成后禁止了在 master 运行任务，防止 master 负荷过大导致集群瘫痪\n  `kubectl taint node mymasternode node-role.kubernetes.io/master:NoSchedule`\n\n#### 安装 Rancher\n\n首先安装 helm，下载到 `/usr/local/bin/helm`\n\n安装 rancher\n\n```shell\nhelm repo add rancher  https://releases.rancher.com/server-charts/latest\nhelm repo update\nhelm install rancher rancher-stable/rancher --namespace cattle-system\n```\n\n配置好域名证书后，正常访问\n\n#### NFS\n\n希望集群能够共享数据，所以增加了 nfs 存储，可以在节点之间共享数据\n\n- 配置 NFS 服务器\n\n方便管理，将所有 export 都放入一个目录，真正的目录都通过`--bind`来绑定\n\n```shell\nmkdir -p /export/volumes\n# 配置权限\nchmod 777 -R /export\n\nmount --bind /home/nfs-data /export/volumes\n```\n\n在/etc/fstab 中添加\n\n```fstab\n/home/nfs-data    /export/volumes   none    bind  0  0\n```\n\n在`/etc/exports`中配置\n\n```exports\n/export/volumes 10.1.108.0/24(rw,nohide,subtree_check,insecure,all_squash,anonuid=0,async)\n\n/export 10.1.0.0/16(ro,fsid=0,root_squash,no_subtree_check,hide)\n```\n\n#### 安装 nfs-client-provisioner\n\n使用 helm 安装 nfs-client-provisioner，为集群提供动态存储\n\n```shell\nhelm install -n kube-system nfs-client-provisioner --set nfs.server={nfs-server-ip} --set nfs.path=/volumes stable/nfs-client-provisioner --set storageClass.name=nfs\n```\n\n#### 安装 Tekton\n\n配置安装 Tekton\n\n前置工作已经完成，使用 github 上的项目来安装\n\n下载目录下文件 `https://github.com/gsmlg/pipeline/tree/master/tektoncd`\n\n直接安装即可\n\n需要配置项目\n\n- ssh-key 配置 known_hosts 和 ssh-privatekey\n\n获取权限\n\n这个项目中定义了一个 pipeline，用于跑项目任务\n\n在 rancher 和 gitlab 配置好 eventlistener 的触发 URL，就可以自动运行当前的\npipeline 了\n\n### 总结问题\n\n过程中碰到 too many open files 的问题，修改 ulimit 解决\n\n测试网络和服务器不太稳定，经常连接不上，或者运行两个任务后，系统很卡，主要是虚拟\n平台给的性能严重不足\n\nnfs 服务器和集群机器之间网络有问题，导致无法共享存储，最后被迫使用了\nlocal-path，失去了共享存储的并行任务能力\n\n碰到 tekton 升级 0.12 版本，更新了 git 资源到 task 来实现共享工作空间\n\n给 nfs，tekton，rancher 增加了 toleration 和 affinity，将他们调度到 master，防止\nworker 过于繁忙，导致的无法服务问题\n"}},"__N_SSG":true},"page":"/blogs/[slug]","query":{"slug":"devops-cluster"},"buildId":"oTnaln6XuL20mMnIg8gCL","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
{"pageProps":{"slug":"devops-cluster","blog":{"author":"Gao","content":"最近配置了一个 DevOps 集群给前端使用，现在整理一下这个流程，和在做成碰到的问题\n\n### 安装\n\n#### 集群\n\n由于要配置多台服务器，安装使用离线安装，来减轻现在的负担\n\n下载好 k3s 和镜像包\n\n- master:\n\n```bash\ncp ./k3s /usr/local/bin/\ncp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/\n\ncat ./install.sh | \\\nINSTALL_K3S_SKIP_DOWNLOAD=true \\\n    sh -s - server \\\n    --cluster-domain=devops.local \\\n    --cluster-cidr=10.31.0.0/16 \\\n    --service-cidr=10.32.0.0/16 \\\n    --default-local-storage-path=/storage/volumes \\\n    --node-name=master \\\n    --docker\n```\n\n- backup:\n\n```bash\nmkdir -p /var/lib/rancher/k3s/agent/images/\ncp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/\n\nexport K3S_TOKEN=\"{server-token}\"\nexport K3S_URL=\"https://{server-ip}:6443\"\n\ncat ./install.sh | \\\nINSTALL_K3S_SKIP_DOWNLOAD=true \\\n    sh -s - agent \\\n    --docker \\\n    --node-name=slave \\\n```\n\n- ⚠️ 注意问题\n\n* 安装好后集群运行有问题，发现是 iptables 设置问题处理方式\n\n```bash\n# master\niptables -F && systemctl restart k3s\n# slave\niptables -F && systemctl restart k3s-agent\n```\n\n如果服务重启或者网络重启，可能会导致问题，也需要这么处理\n\n- 这里使用 docker 而没有使用 containerd 是因为网络环境有问题，rancher-agent 无法\n  运行导致的\n- 安装完成后禁止了在 master 运行任务，防止 master 负荷过大导致集群瘫痪\n  `kubectl taint node mymasternode node-role.kubernetes.io/master:NoSchedule`\n\n#### 安装 Rancher\n\n首先安装 helm，下载到 `/usr/local/bin/helm`\n\n安装 rancher\n\n```shell\nhelm repo add rancher  https://releases.rancher.com/server-charts/latest\nhelm repo update\nhelm install rancher rancher-stable/rancher --namespace cattle-system\n```\n\n配置好域名证书后，正常访问\n\n#### NFS\n\n希望集群能够共享数据，所以增加了 nfs 存储，可以在节点之间共享数据\n\n- 配置 NFS 服务器\n\n方便管理，将所有 export 都放入一个目录，真正的目录都通过`--bind`来绑定\n\n```shell\nmkdir -p /export/volumes\n# 配置权限\nchmod 777 -R /export\n\nmount --bind /home/nfs-data /export/volumes\n```\n\n在/etc/fstab 中添加\n\n```fstab\n/home/nfs-data    /export/volumes   none    bind  0  0\n```\n\n在`/etc/exports`中配置\n\n```exports\n/export/volumes 10.1.108.0/24(rw,nohide,subtree_check,insecure,all_squash,anonuid=0,async)\n\n/export 10.1.0.0/16(ro,fsid=0,root_squash,no_subtree_check,hide)\n```\n\n#### 安装 nfs-client-provisioner\n\n使用 helm 安装 nfs-client-provisioner，为集群提供动态存储\n\n```shell\nhelm install -n kube-system nfs-client-provisioner --set nfs.server={nfs-server-ip} --set nfs.path=/volumes stable/nfs-client-provisioner --set storageClass.name=nfs\n```\n\n#### 安装 Tekton\n\n配置安装 Tekton\n\n前置工作已经完成，使用 github 上的项目来安装\n\n下载目录下文件 `https://github.com/gsmlg/pipeline/tree/master/tektoncd`\n\n直接安装即可\n\n需要配置项目\n\n- ssh-key 配置 known_hosts 和 ssh-privatekey\n\n获取权限\n\n这个项目中定义了一个 pipeline，用于跑项目任务\n\n在 rancher 和 gitlab 配置好 eventlistener 的触发 URL，就可以自动运行当前的\npipeline 了\n\n### 总结问题\n\n过程中碰到 too many open files 的问题，修改 ulimit 解决\n\n测试网络和服务器不太稳定，经常连接不上，或者运行两个任务后，系统很卡，主要是虚拟\n平台给的性能严重不足\n\nnfs 服务器和集群机器之间网络有问题，导致无法共享存储，最后被迫使用了\nlocal-path，失去了共享存储的并行任务能力\n\n碰到 tekton 升级 0.12 版本，更新了 git 资源到 task 来实现共享工作空间\n\n给 nfs，tekton，rancher 增加了 toleration 和 affinity，将他们调度到 master，防止\nworker 过于繁忙，导致的无法服务问题\n","date":"2020-05-12","id":37,"slug":"devops-cluster","title":"DevOps集群"}},"__N_SSG":true}